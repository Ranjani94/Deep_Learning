{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Date_translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3mfsmv2duFeM3ux2XCNeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani94/Deep_Learning/blob/master/Graded_Assignment_5/Date_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbuXLzJXebQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "# from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn-MRME0eePK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "da9e2d1a-1783-4b09-e7a2-1377cd17c984"
      },
      "source": [
        "pip install Faker"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/9d/39cc13537ba25a7819bd566d0b0cddfa5570579c194756ac3aff57256dcd/Faker-4.1.0-py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->Faker) (1.12.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqdzDPDez55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fake = Faker()\n",
        "# fake.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n",
        "\n",
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine\n",
        "\n",
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l\n",
        "\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
        "\n",
        "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
        "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
        "    prediction = model.predict(np.array([encoded]))\n",
        "    prediction = np.argmax(prediction[0], axis=-1)\n",
        "    return int_to_string(prediction, inv_output_vocabulary)\n",
        "\n",
        "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
        "    predicted = []\n",
        "    for example in examples:\n",
        "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
        "        print('input:', example)\n",
        "        print('output:', predicted[-1])\n",
        "    return predicted\n",
        "\n",
        "\n",
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "        \n",
        "\n",
        "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
        "    \"\"\"\n",
        "    Plot the attention map.\n",
        "  \n",
        "    \"\"\"\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    Ty, Tx = attention_map.shape\n",
        "    \n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    layer = model.layers[num]\n",
        "\n",
        "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    r = f([encoded, s0, c0])\n",
        "    \n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
        "\n",
        "    # Normalize attention map\n",
        "#     row_max = attention_map.max(axis=1)\n",
        "#     attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = model.predict([encoded, s0, c0])\n",
        "    \n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "        \n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "    \n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "    \n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "    \n",
        "    return attention_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sej_UGyyfWOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74e01c83-c788-43ee-a241-ce10a847f49b"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 21196.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeTHYKOJfkpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b4d7d31c-9170-4207-bf7b-aed58a052189"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbJzXwqvfpHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "127f8e9b-02b5-4819-bfe9-030dcc0fd4cd"
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: 17 jul 1980\n",
            "Target date: 1980-07-17\n",
            "\n",
            "Source after preprocessing (indices): [ 4 10  0 22 31 23  0  4 12 11  3 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10  9  1  0  1  8  0  2  8]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtchrm7XfrXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCubPfTfuQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    e = densor2(e)\n",
        "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(e)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpHrotZxfwCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "n_a = 32\n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkOSR913fx_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    \n",
        "    # Step 2: Iterate \n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uIL2jlQf5Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjUOSIpcf7Hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fedb91f1-72f1-4977-889a-ae79a176c294"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tjhTD_if889",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
        "                    metrics=['accuracy'],\n",
        "                    loss='categorical_crossentropy')\n",
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_6dsjOZgACU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz_yTJjIgBkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a78f42cc-832a-4a9b-dfd4-2d3ec6cfc087"
      },
      "source": [
        "\n",
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 16.7336 - dense_3_loss: 2.6051 - dense_3_accuracy: 0.5791 - dense_3_accuracy_1: 0.6829 - dense_3_accuracy_2: 0.3028 - dense_3_accuracy_3: 0.0713 - dense_3_accuracy_4: 0.9021 - dense_3_accuracy_5: 0.3163 - dense_3_accuracy_6: 0.0468 - dense_3_accuracy_7: 0.9049 - dense_3_accuracy_8: 0.2569 - dense_3_accuracy_9: 0.1080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcd9976a860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT7Y1PZBgDBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4332968-e1e4-4911-f66a-55f7907b25d6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDEksDEvkt2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "992e8f2d-b46b-4397-b1e1-03e65c22f898"
      },
      "source": [
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x612 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBBqEeBxk1EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}